{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries for most tasks.\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# sklearn libraries.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# keras libraries.\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to postpreprocessed, part-of-speech tagged review corpus\n",
    "# {\"cat\": \"sports\", \"txt\": \"Barely better than Gabbert?\"}\n",
    "jsonFile = pd.read_json(\"categorized-comments.jsonl\", lines =  True)\n",
    "\n",
    "stopwords = set(nk.corpus.stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "lemmatizeList = []\n",
    "\n",
    "for index in range(0, len(jsonFile)):\n",
    "    processed_tokens = word_tokenize(jsonFile[\"txt\"][index])\n",
    "    processed_tokens = [w.lower() for w in processed_tokens]\n",
    "    processed_tokens = [w for w in processed_tokens if w not in stopwords]\n",
    "    processed_tokens = [lemmatizer.lemmatize(w) for w in processed_tokens]\n",
    "    lemmatizeList.append(processed_tokens)       \n",
    "\n",
    "jsonFile[\"lemmatize\"] = lemmatizeList\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "      <th>lemmatize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>Barely better than Gabbert? He was significant...</td>\n",
       "      <td>[barely, better, gabbert, ?, significantly, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n",
       "      <td>[fuck, duck, angel, !, welcome, new, niner, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n",
       "      <td>[drafted, wrs, ., -, matt, millen, probably]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n",
       "      <td>[[, done, ], (, http, :, //i.imgur.com/2yz90pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>No!! NOO!!!!!</td>\n",
       "      <td>[!, !, noo, !, !, !, !, !]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt  \\\n",
       "0  sports  Barely better than Gabbert? He was significant...   \n",
       "1  sports  Fuck the ducks and the Angels! But welcome to ...   \n",
       "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...   \n",
       "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)   \n",
       "4  sports                                      No!! NOO!!!!!   \n",
       "\n",
       "                                           lemmatize  \n",
       "0  [barely, better, gabbert, ?, significantly, be...  \n",
       "1  [fuck, duck, angel, !, welcome, new, niner, fa...  \n",
       "2       [drafted, wrs, ., -, matt, millen, probably]  \n",
       "3  [[, done, ], (, http, :, //i.imgur.com/2yz90pm...  \n",
       "4                         [!, !, noo, !, !, !, !, !]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonFile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21596/3054625987.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "train_test_split(X, y, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import nltk\n",
    "import pickle\n",
    "import sqlite3\n",
    "\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "\n",
    "PKL_PATTERN = r'(?!\\.)[\\w\\s\\d\\-]+\\.pickle'\n",
    "\n",
    "class SqliteCorpusReader(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self._cur = sqlite3.connect(path).cursor()\n",
    "\n",
    "    def scores(self):\n",
    "        \"\"\"\n",
    "        Returns the review score\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT score FROM reviews\")\n",
    "        scores = self._cur.fetchall()\n",
    "        for score in scores:\n",
    "            yield score\n",
    "\n",
    "    def texts(self):\n",
    "        \"\"\"\n",
    "        Returns the full review texts\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT content FROM content\")\n",
    "        texts = self._cur.fetchall()\n",
    "        for text in texts:\n",
    "            yield text\n",
    "\n",
    "    def ids(self):\n",
    "        \"\"\"\n",
    "        Returns the review ids\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT reviewid FROM content\")\n",
    "        ids = self._cur.fetchall()\n",
    "        for idx in ids:\n",
    "            yield idx\n",
    "\n",
    "    def ids_and_texts(self):\n",
    "        \"\"\"\n",
    "        Returns the review ids\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM content\")\n",
    "        results = self._cur.fetchall()\n",
    "        for idx,text in results:\n",
    "            yield idx,text\n",
    "\n",
    "    def scores_albums_artists_texts(self):\n",
    "        \"\"\"\n",
    "        Returns a generator with each review represented as a\n",
    "        (score, album name, artist name, review text) tuple\n",
    "        \"\"\"\n",
    "        sql = \"\"\"\n",
    "              SELECT S.score, L.label, A.artist, R.content\n",
    "              FROM [reviews] S\n",
    "              JOIN labels L ON S.reviewid=L.reviewid\n",
    "              JOIN artists A on L.reviewid=A.reviewid\n",
    "              JOIN content R ON A.reviewid=R.reviewid\n",
    "              \"\"\"\n",
    "        self._cur.execute(sql)\n",
    "        results = self._cur.fetchall()\n",
    "        for score,album,band,text in results:\n",
    "            yield (score,album,band,text)\n",
    "\n",
    "    def albums(self):\n",
    "        \"\"\"\n",
    "        Returns the names of albums being reviewed\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM labels\")\n",
    "        albums = self._cur.fetchall()\n",
    "        for idx,album in albums:\n",
    "            yield idx,album\n",
    "\n",
    "    def artists(self):\n",
    "        \"\"\"\n",
    "        Returns the name of the artist being reviewed\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM artists\")\n",
    "        artists = self._cur.fetchall()\n",
    "        for idx,artist in artists:\n",
    "            yield idx,artist\n",
    "\n",
    "    def genres(self):\n",
    "        \"\"\"\n",
    "        Returns the music genre of each review\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM genres\")\n",
    "        genres = self._cur.fetchall()\n",
    "        for idx,genre in genres:\n",
    "            yield idx,genre\n",
    "\n",
    "    def years(self):\n",
    "        \"\"\"\n",
    "        Returns the publication year of each review\n",
    "        Note: There are many missing values\n",
    "        \"\"\"\n",
    "        self._cur.execute(\"SELECT * FROM years\")\n",
    "        years = self._cur.fetchall()\n",
    "        for idx,year in years:\n",
    "            yield idx,year\n",
    "\n",
    "    def paras(self):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs.\n",
    "        \"\"\"\n",
    "        for text in self.texts():\n",
    "            for paragraph in text:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences.\n",
    "        \"\"\"\n",
    "        for para in self.paras():\n",
    "            for sentence in nltk.sent_tokenize(para):\n",
    "                yield sentence\n",
    "\n",
    "    def words(self):\n",
    "        \"\"\"\n",
    "        Returns a generator of words.\n",
    "        \"\"\"\n",
    "        for sent in self.sents():\n",
    "            for word in nltk.wordpunct_tokenize(sent):\n",
    "                yield word\n",
    "\n",
    "    def tagged_tokens(self):\n",
    "        for sent in self.sents():\n",
    "            for word in nltk.wordpunct_tokenize(sent):\n",
    "                yield nltk.pos_tag(word)\n",
    "\n",
    "\n",
    "\n",
    "class PickledReviewsReader(CorpusReader):\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader\n",
    "        \"\"\"\n",
    "        CorpusReader.__init__(self, root, fileids, **kwargs)\n",
    "\n",
    "    def texts_scores(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the SqliteCorpusReader, this uses a generator\n",
    "        to achieve memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def reviews(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for text,score in self.texts_scores(fileids):\n",
    "            yield text\n",
    "\n",
    "    def scores(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Return the scores\n",
    "        \"\"\"\n",
    "        for text,score in self.texts_scores(fileids):\n",
    "            yield score\n",
    "\n",
    "    def paras(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for review in self.reviews(fileids):\n",
    "            for paragraph in review:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids):\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "\n",
    "    def tagged(self, fileids=None):\n",
    "        for sent in self.sents(fileids):\n",
    "            for token in sent:\n",
    "                yield token\n",
    "\n",
    "    def words(self, fileids=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for token in self.tagged(fileids):\n",
    "            yield token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import nltk\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token)\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopword\n",
    "    \n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token) and not self.is_stopword(token)]\n",
    "    \n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "             }.get(pos_tag[0], wn.NOUN)\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.normalize(document)\n",
    "\n",
    "\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def documents(corpus):\n",
    "    return list(corpus.reviews())\n",
    "\n",
    "def continuous(corpus):\n",
    "    return list(corpus.scores())\n",
    "\n",
    "def make_categorical(corpus):\n",
    "    return np.digitize(continuous(corpus), [0.0, 3.0, 5.0, 7.0, 10.1])\n",
    "\n",
    "def binarize(corpus):\n",
    "    return np.digitize(continuous(corpus), [0.0, 3.0, 5.1])\n",
    "\n",
    "def train_model(path, model, Continuous=True, saveto=None, cv=12):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path; constructing cross-validation\n",
    "    scores using the cv parameter, then fitting the model on the full data.\n",
    "    Returns the scores.\n",
    "    \"\"\"\n",
    "    # Load the corpus data and labels for classification\n",
    "    comments = pd.read_json(r'C:\\Users\\danie\\Documents\\School\\DSC-550\\Week2\\controversial-comments.jsonl', lines=True)\n",
    "    \n",
    "    corpus = PickledReviewsReader(comments.to_csv(sep='\\t'))\n",
    "    X = documents(corpus)\n",
    "    \n",
    "    print(corpus)\n",
    "    \n",
    "    if Continuous:\n",
    "        y = continuous(corpus)\n",
    "        scoring = 'r2'\n",
    "        \n",
    "    else:\n",
    "        y = make_categorical(corpus)\n",
    "        scoring = 'f1'\n",
    "    \n",
    "    # Compute cross-validation scores\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "    \n",
    "    # Write to disk if specified\n",
    "    if saveto:\n",
    "        joblib.dump(model, saveto)\n",
    "    # Fit the model on entire dataset\n",
    "    model.fit(X, y)\n",
    "    # Return scores\n",
    "    return score\n",
    "\n",
    "cpath = 'categorized-comments.txt'\n",
    "\n",
    "regressor = Pipeline([\n",
    " ('norm', TextNormalizer()),\n",
    " ('tfidf', TfidfVectorizer()),\n",
    " ('ann', MLPRegressor(hidden_layer_sizes=[500,150], verbose=True))\n",
    " ])\n",
    "\n",
    "regression_scores = train_model(cpath, regressor, Continuous=True)\n",
    "\n",
    "classifier = Pipeline([\n",
    "('norm', TextNormalizer()),\n",
    "('tfidf', TfidfVectorizer()),\n",
    "('ann', MLPClassifier(hidden_layer_sizes=[500,150], verbose=True))\n",
    "])\n",
    "\n",
    "classifer_scores = train_model(cpath, classifier, Continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regression_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21596/1957217107.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mregression_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'regression_scores' is not defined"
     ]
    }
   ],
   "source": [
    "regression_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifer_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21596/236514205.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifer_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classifer_scores' is not defined"
     ]
    }
   ],
   "source": [
    "classifer_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c96f079fa0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Set the number of features we want\n",
    "number_of_features = 5000\n",
    "# Load feature and target data\n",
    "data = reuters.load_data(num_words=number_of_features)\n",
    "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
    "# Convert feature data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "# One-hot encode target vector to create a target matrix\n",
    "target_train = to_categorical(target_vector_train)\n",
    "target_test = to_categorical(target_vector_test)\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100,\n",
    " activation=\"relu\",\n",
    "input_shape=(number_of_features,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    " optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    " metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    " target_train, # Target\n",
    "epochs=3, # Three epochs\n",
    " verbose=0, # No output\n",
    " batch_size=100, # Number of observations per batch\n",
    " validation_data=(features_test, target_test)) # Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
